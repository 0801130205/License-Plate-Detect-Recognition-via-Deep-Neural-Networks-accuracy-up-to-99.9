Log file created at: 2018/06/14 09:54:19
Running on machine: ZBF-PC
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0614 09:54:19.118979  8840 caffe.cpp:213] CUDNN version: 5110
I0614 09:54:19.262946  8840 caffe.cpp:229] Using GPUs 0
I0614 09:54:19.262946  8840 caffe.cpp:234] GPU 0: GeForce GTX 1070
I0614 09:54:19.544226  8840 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 0.0001
display: 200
max_iter: 40000000
lr_policy: "fixed"
gamma: 0.5
momentum: 0.9
weight_decay: 0.0001
snapshot: 20000
snapshot_prefix: "densenet-bigger-5x5-no-lstm"
solver_mode: GPU
device_id: 0
random_seed: 1234
net: "C:/WM_LSTM/train_tools/densenet-sum-blstm-full-res-blstm_train-val.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 300000
stepvalue: 600000
stepvalue: 900000
stepvalue: 1200000
stepvalue: 1500000
stepvalue: 1800000
type: "Nesterov"
I0614 09:54:19.544226  8840 solver.cpp:91] Creating training net from net file: C:/WM_LSTM/train_tools/densenet-sum-blstm-full-res-blstm_train-val.prototxt
I0614 09:54:19.544226  8840 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0614 09:54:19.544226  8840 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer acc
I0614 09:54:19.544226  8840 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_value: 152
    mean_value: 152
    mean_value: 152
  }
  image_data_param {
    source: "C:\\WM_LSTM\\13194439332.txt"
    batch_size: 96
    shuffle: true
    new_height: 32
    new_width: 280
    is_color: true
    root_folder: "C:\\WM_LSTM\\training_data\\"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "DenseBlock1"
  type: "DenseBlock"
  bottom: "conv1"
  top: "DenseBlock1"
  denseblock_param {
    numTransition: 8
    initChannel: 64
    growthRate: 8
    Filter_Filler {
      type: "msra"
    }
    BN_Scaler_Filler {
      type: "constant"
      value: 1
    }
    BN_Bias_Filler {
      type: "constant"
      value: 0
    }
    use_dropout: false
    dropout_amount: 0.2
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "DenseBlock1"
  top: "BatchNorm1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "BatchNorm1"
  top: "Convolution2"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Dropout1"
  type: "Dropout"
  bottom: "Convolution2"
  top: "Dropout1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Dropout1"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "DenseBlock2"
  type: "DenseBlock"
  bottom: "Pooling1"
  top: "DenseBlock2"
  denseblock_param {
    numTransition: 8
    initChannel: 64
    growthRate: 8
    Filter_Filler {
      type: "msra"
    }
    BN_Scaler_Filler {
      type: "constant"
      value: 1
    }
    BN_Bias_Filler {
      type: "constant"
      value: 0
    }
    use_dropout: false
    dropout_amount: 0.2
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "DenseBlock2"
  top: "BatchNorm2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "BatchNorm2"
  top: "Convolution3"
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Dropout2"
  type: "Dropout"
  bottom: "Convolution3"
  top: "Convolution3"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling2"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "DenseBlock3"
  type: "DenseBlock"
  bottom: "Pooling2"
  top: "DenseBlock3"
  denseblock_param {
    numTransition: 8
    initChannel: 64
    growthRate: 8
    Filter_Filler {
      type: "msra"
    }
    BN_Scaler_Filler {
      type: "constant"
      value: 1
    }
    BN_Bias_Filler {
      type: "constant"
      value: 0
    }
    use_dropout: false
    dropout_amount: 0.2
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "DenseBlock3"
  top: "BatchNorm3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
}
layer {
  name: "pool5_ave"
  type: "Pooling"
  bottom: "BatchNorm3"
  top: "pool5_ave"
  pooling_param {
    pool: AVE
    kernel_h: 4
    kernel_w: 1
    stride_h: 1
    stride_w: 1
  }
}
layer {
  name: "pool5_ave_transpose"
  type: "Transpose"
  bottom: "pool5_ave"
  top: "pool5_ave_transpose"
  transpose_param {
    dim: 3
    dim: 2
    dim: 0
    dim: 1
  }
}
layer {
  name: "blstm_input"
  type: "Reshape"
  bottom: "pool5_ave_transpose"
  top: "blstm_input"
  reshape_param {
    shape {
      dim: -1
    }
    axis: 1
    num_axes: 2
  }
}
layer {
  name: "lstm1"
  type: "Lstm"
  bottom: "blstm_input"
  top: "lstm1"
  lstm_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "lstm1-reverse1"
  type: "Reverse"
  bottom: "blstm_input"
  top: "rlstm1_input"
  reverse_param {
    axis: 0
  }
}
layer {
  name: "rlstm1"
  type: "Lstm"
  bottom: "rlstm1_input"
  top: "rlstm1-output"
  lstm_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "lstm1-reverse2"
  type: "Reverse"
  bottom: "rlstm1-output"
  top: "rlstm1"
  reverse_param {
    axis: 0
  }
}
layer {
  name: "blstm1"
  type: "Eltwise"
  bottom: "lstm1"
  bottom: "rlstm1"
  bottom: "blstm_input"
  top: "blstm1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "lstm2"
  type: "Lstm"
  bottom: "blstm1"
  top: "lstm2"
  lstm_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "lstm2-reverse1"
  type: "Reverse"
  bottom: "blstm1"
  top: "rlstm2_input"
  reverse_param {
    axis: 0
  }
}
layer {
  name: "rlstm2"
  type: "Lstm"
  bottom: "rlstm2_input"
  top: "rlstm2-output"
  lstm_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "lstm2-reverse2"
  type: "Reverse"
  bottom: "rlstm2-output"
  top: "rlstm2"
  reverse_param {
    axis: 0
  }
}
layer {
  name: "blstm2"
  type: "Eltwise"
  bottom: "lstm2"
  bottom: "rlstm2"
  bottom: "blstm1"
  bottom: "blstm_input"
  top: "blstm2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "fc1x"
  type: "InnerProduct"
  bottom: "blstm2"
  top: "fc1x"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 69
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    axis: 2
  }
}
layer {
  name: "ctcloss"
  type: "WarpCTCLoss"
  bottom: "fc1x"
  bottom: "label"
  top: "ctcloss"
  loss_weight: 1
}
I0614 09:54:19.544226  8840 layer_factory.hpp:77] Creating layer data
I0614 09:54:19.544226  8840 net.cpp:100] Creating Layer data
I0614 09:54:19.544226  8840 net.cpp:408] data -> data
I0614 09:54:19.544226  8840 net.cpp:408] data -> label
I0614 09:54:19.544226  8840 image_data_layer.cpp:76] Opening file C:\WM_LSTM\13194439332.txt
I0614 09:54:22.306758  8840 image_data_layer.cpp:98] Shuffling data
I0614 09:54:22.338013  8840 image_data_layer.cpp:103] A total of 813501 images.
I0614 09:54:22.369240  8840 image_data_layer.cpp:130] output data size: 96,3,32,280
I0614 09:54:22.384893  8840 net.cpp:150] Setting up data
I0614 09:54:22.384893  8840 net.cpp:157] Top shape: 96 3 32 280 (2580480)
I0614 09:54:22.384893  8840 net.cpp:157] Top shape: 96 5 1 1 (480)
I0614 09:54:22.400493  8840 net.cpp:165] Memory required for data: 10323840
I0614 09:54:22.400493  8840 layer_factory.hpp:77] Creating layer conv1
I0614 09:54:22.400493  8840 net.cpp:100] Creating Layer conv1
I0614 09:54:22.400493  8840 net.cpp:434] conv1 <- data
I0614 09:54:22.400493  8840 net.cpp:408] conv1 -> conv1
E0614 09:54:22.416118 15796 io.cpp:82] Could not open or find file C:\WM_LSTM\training_data\13195158296_1348_24-1-1-1-1-4-.jpg
F0614 09:54:22.416118 15796 image_data_layer.cpp:263] Check failed: cv_img.data Could not load 13195158296_1348_24-1-1-1-1-4-.jpg
