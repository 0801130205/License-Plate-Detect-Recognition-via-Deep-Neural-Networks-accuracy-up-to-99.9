Log file created at: 2018/06/15 12:54:31
Running on machine: ZBF-PC
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0615 12:54:31.240991 20468 caffe.cpp:213] CUDNN version: 5110
I0615 12:54:31.443554 20468 caffe.cpp:229] Using GPUs 0
I0615 12:54:31.445536 20468 caffe.cpp:234] GPU 0: GeForce GTX 1070
I0615 12:54:31.990013 20468 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 0.001
display: 200
max_iter: 40000000
lr_policy: "fixed"
gamma: 0.5
momentum: 0.9
weight_decay: 0.0001
snapshot: 20000
snapshot_prefix: "densenet-bigger-5x5-no-lstm"
solver_mode: GPU
device_id: 0
random_seed: 1234
net: "C:/WM_LSTM/train_tools/densenet-sum-blstm-full-res-blstm_train-val.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 300000
stepvalue: 600000
stepvalue: 900000
stepvalue: 1200000
stepvalue: 1500000
stepvalue: 1800000
type: "Nesterov"
I0615 12:54:31.991988 20468 solver.cpp:91] Creating training net from net file: C:/WM_LSTM/train_tools/densenet-sum-blstm-full-res-blstm_train-val.prototxt
I0615 12:54:31.992992 20468 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0615 12:54:31.992992 20468 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer acc
I0615 12:54:31.993995 20468 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_value: 152
    mean_value: 152
    mean_value: 152
  }
  image_data_param {
    source: "C:\\WM_LSTM\\1410450762.txt"
    batch_size: 96
    shuffle: true
    new_height: 32
    new_width: 280
    is_color: true
    root_folder: "C:\\WM_LSTM\\train_data1\\"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "DenseBlock1"
  type: "DenseBlock"
  bottom: "conv1"
  top: "DenseBlock1"
  denseblock_param {
    numTransition: 8
    initChannel: 64
    growthRate: 8
    Filter_Filler {
      type: "msra"
    }
    BN_Scaler_Filler {
      type: "constant"
      value: 1
    }
    BN_Bias_Filler {
      type: "constant"
      value: 0
    }
    use_dropout: false
    dropout_amount: 0.2
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "DenseBlock1"
  top: "BatchNorm1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "BatchNorm1"
  top: "Convolution2"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Dropout1"
  type: "Dropout"
  bottom: "Convolution2"
  top: "Dropout1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Dropout1"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "DenseBlock2"
  type: "DenseBlock"
  bottom: "Pooling1"
  top: "DenseBlock2"
  denseblock_param {
    numTransition: 8
    initChannel: 64
    growthRate: 8
    Filter_Filler {
      type: "msra"
    }
    BN_Scaler_Filler {
      type: "constant"
      value: 1
    }
    BN_Bias_Filler {
      type: "constant"
      value: 0
    }
    use_dropout: false
    dropout_amount: 0.2
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "DenseBlock2"
  top: "BatchNorm2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "BatchNorm2"
  top: "Convolution3"
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Dropout2"
  type: "Dropout"
  bottom: "Convolution3"
  top: "Convolution3"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling2"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "DenseBlock3"
  type: "DenseBlock"
  bottom: "Pooling2"
  top: "DenseBlock3"
  denseblock_param {
    numTransition: 8
    initChannel: 64
    growthRate: 8
    Filter_Filler {
      type: "msra"
    }
    BN_Scaler_Filler {
      type: "constant"
      value: 1
    }
    BN_Bias_Filler {
      type: "constant"
      value: 0
    }
    use_dropout: false
    dropout_amount: 0.2
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "DenseBlock3"
  top: "BatchNorm3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
}
layer {
  name: "pool5_ave"
  type: "Pooling"
  bottom: "BatchNorm3"
  top: "pool5_ave"
  pooling_param {
    pool: AVE
    kernel_h: 4
    kernel_w: 1
    stride_h: 1
    stride_w: 1
  }
}
layer {
  name: "pool5_ave_transpose"
  type: "Transpose"
  bottom: "pool5_ave"
  top: "pool5_ave_transpose"
  transpose_param {
    dim: 3
    dim: 2
    dim: 0
    dim: 1
  }
}
layer {
  name: "blstm_input"
  type: "Reshape"
  bottom: "pool5_ave_transpose"
  top: "blstm_input"
  reshape_param {
    shape {
      dim: -1
    }
    axis: 1
    num_axes: 2
  }
}
layer {
  name: "lstm1"
  type: "Lstm"
  bottom: "blstm_input"
  top: "lstm1"
  lstm_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "lstm1-reverse1"
  type: "Reverse"
  bottom: "blstm_input"
  top: "rlstm1_input"
  reverse_param {
    axis: 0
  }
}
layer {
  name: "rlstm1"
  type: "Lstm"
  bottom: "rlstm1_input"
  top: "rlstm1-output"
  lstm_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "lstm1-reverse2"
  type: "Reverse"
  bottom: "rlstm1-output"
  top: "rlstm1"
  reverse_param {
    axis: 0
  }
}
layer {
  name: "blstm1"
  type: "Eltwise"
  bottom: "lstm1"
  bottom: "rlstm1"
  bottom: "blstm_input"
  top: "blstm1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "lstm2"
  type: "Lstm"
  bottom: "blstm1"
  top: "lstm2"
  lstm_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "lstm2-reverse1"
  type: "Reverse"
  bottom: "blstm1"
  top: "rlstm2_input"
  reverse_param {
    axis: 0
  }
}
layer {
  name: "rlstm2"
  type: "Lstm"
  bottom: "rlstm2_input"
  top: "rlstm2-output"
  lstm_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "lstm2-reverse2"
  type: "Reverse"
  bottom: "rlstm2-output"
  top: "rlstm2"
  reverse_param {
    axis: 0
  }
}
layer {
  name: "blstm2"
  type: "Eltwise"
  bottom: "lstm2"
  bottom: "rlstm2"
  bottom: "blstm1"
  bottom: "blstm_input"
  top: "blstm2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "fc1x"
  type: "InnerProduct"
  bottom: "blstm2"
  top: "fc1x"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 21
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    axis: 2
  }
}
layer {
  name: "ctcloss"
  type: "WarpCTCLoss"
  bottom: "fc1x"
  bottom: "label"
  top: "ctcloss"
  loss_weight: 1
}
I0615 12:54:32.001013 20468 layer_factory.hpp:77] Creating layer data
I0615 12:54:32.002015 20468 net.cpp:100] Creating Layer data
I0615 12:54:32.002015 20468 net.cpp:408] data -> data
I0615 12:54:32.003018 20468 net.cpp:408] data -> label
I0615 12:54:32.003018 20468 image_data_layer.cpp:76] Opening file C:\WM_LSTM\1410450762.txt
I0615 12:54:34.729266 20468 image_data_layer.cpp:98] Shuffling data
I0615 12:54:34.774387 20468 image_data_layer.cpp:103] A total of 767659 images.
I0615 12:54:34.818505 20468 image_data_layer.cpp:130] output data size: 96,3,32,280
I0615 12:54:34.838557 20468 net.cpp:150] Setting up data
I0615 12:54:34.838557 20468 net.cpp:157] Top shape: 96 3 32 280 (2580480)
I0615 12:54:34.840564 20468 net.cpp:157] Top shape: 96 5 1 1 (480)
I0615 12:54:34.840564 20468 net.cpp:165] Memory required for data: 10323840
I0615 12:54:34.840564 20468 layer_factory.hpp:77] Creating layer conv1
I0615 12:54:34.841565 20468 net.cpp:100] Creating Layer conv1
I0615 12:54:34.841565 20468 net.cpp:434] conv1 <- data
I0615 12:54:34.841565 20468 net.cpp:408] conv1 -> conv1
I0615 12:54:36.174109 20468 net.cpp:150] Setting up conv1
I0615 12:54:36.174109 20468 net.cpp:157] Top shape: 96 64 16 140 (13762560)
I0615 12:54:36.175112 20468 net.cpp:165] Memory required for data: 65374080
I0615 12:54:36.175112 20468 layer_factory.hpp:77] Creating layer DenseBlock1
I0615 12:54:36.176115 20468 net.cpp:100] Creating Layer DenseBlock1
I0615 12:54:36.177117 20468 net.cpp:434] DenseBlock1 <- conv1
I0615 12:54:36.177117 20468 net.cpp:408] DenseBlock1 -> DenseBlock1
I0615 12:54:36.212211 20468 net.cpp:150] Setting up DenseBlock1
I0615 12:54:36.212211 20468 net.cpp:157] Top shape: 96 128 16 140 (27525120)
I0615 12:54:36.213214 20468 net.cpp:165] Memory required for data: 175474560
I0615 12:54:36.213214 20468 layer_factory.hpp:77] Creating layer BatchNorm1
I0615 12:54:36.214216 20468 net.cpp:100] Creating Layer BatchNorm1
I0615 12:54:36.214216 20468 net.cpp:434] BatchNorm1 <- DenseBlock1
I0615 12:54:36.216223 20468 net.cpp:408] BatchNorm1 -> BatchNorm1
I0615 12:54:36.217224 20468 net.cpp:150] Setting up BatchNorm1
I0615 12:54:36.217224 20468 net.cpp:157] Top shape: 96 128 16 140 (27525120)
I0615 12:54:36.218228 20468 net.cpp:165] Memory required for data: 285575040
I0615 12:54:36.218228 20468 layer_factory.hpp:77] Creating layer Scale1
I0615 12:54:36.219229 20468 net.cpp:100] Creating Layer Scale1
I0615 12:54:36.219229 20468 net.cpp:434] Scale1 <- BatchNorm1
I0615 12:54:36.220233 20468 net.cpp:395] Scale1 -> BatchNorm1 (in-place)
I0615 12:54:36.220233 20468 layer_factory.hpp:77] Creating layer Scale1
I0615 12:54:36.221235 20468 net.cpp:150] Setting up Scale1
I0615 12:54:36.221235 20468 net.cpp:157] Top shape: 96 128 16 140 (27525120)
I0615 12:54:36.222239 20468 net.cpp:165] Memory required for data: 395675520
I0615 12:54:36.223240 20468 layer_factory.hpp:77] Creating layer ReLU1
I0615 12:54:36.224243 20468 net.cpp:100] Creating Layer ReLU1
I0615 12:54:36.224243 20468 net.cpp:434] ReLU1 <- BatchNorm1
I0615 12:54:36.225245 20468 net.cpp:395] ReLU1 -> BatchNorm1 (in-place)
I0615 12:54:36.226248 20468 net.cpp:150] Setting up ReLU1
I0615 12:54:36.227250 20468 net.cpp:157] Top shape: 96 128 16 140 (27525120)
I0615 12:54:36.227250 20468 net.cpp:165] Memory required for data: 505776000
I0615 12:54:36.227250 20468 layer_factory.hpp:77] Creating layer Convolution2
I0615 12:54:36.228253 20468 net.cpp:100] Creating Layer Convolution2
I0615 12:54:36.228253 20468 net.cpp:434] Convolution2 <- BatchNorm1
I0615 12:54:36.229256 20468 net.cpp:408] Convolution2 -> Convolution2
I0615 12:54:36.230258 20468 net.cpp:150] Setting up